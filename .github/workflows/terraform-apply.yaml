name: Terraform - Provision EKS

on:
  push:
    branches:
      - main
    paths:
      - 'terraform/**'
      - '.github/workflows/terraform-apply.yaml'
      - 'k8s/**'
  pull_request:
    branches:
      - main
    paths:
      - 'terraform/**'
      - '.github/workflows/terraform-apply.yaml'
      - 'k8s/**'
  workflow_dispatch:
    inputs:
      action:
        description: 'Terraform action to perform'
        required: true
        default: 'apply'
        type: choice
        options:
          - apply
          - destroy
      debug:
        description: 'Enable debug logging'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TF_VERSION: 1.6.0
  TF_LOG: ${{ inputs.debug && 'DEBUG' || 'INFO' }}
  S3_BUCKET: ${{ secrets.TF_STATE_BUCKET }}  # Move to secrets
  DYNAMODB_TABLE: ${{ secrets.TF_STATE_LOCK_TABLE }}

jobs:
  # Separate job for validation
  validate:
    name: Terraform Validate
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Verify backend resources exist
        run: |
          set -e
          echo "Checking S3 bucket..."
          aws s3api head-bucket --bucket ${{ env.S3_BUCKET }} --region ${{ env.AWS_REGION }}
          
          echo "Checking DynamoDB table..."
          aws dynamodb describe-table --table-name ${{ env.DYNAMODB_TABLE }} --region ${{ env.AWS_REGION }}
      
      - name: Terraform Format Check
        run: |
          terraform fmt -check -recursive
          if [ $? -ne 0 ]; then
            echo "::error::Terraform files are not formatted. Run 'terraform fmt -recursive'"
            exit 1
          fi
      
      - name: Terraform Init
        run: terraform init -backend=true
      
      - name: Terraform Validate
        run: terraform validate

  # Plan job for PRs
  plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: validate
    if: github.event_name == 'pull_request'
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: true  # Enable outputs capture
      
      - name: Terraform Init
        run: terraform init
      
      - name: Terraform Plan
        id: plan
        run: |
          terraform plan -no-color -input=false -out=tfplan
          terraform show -no-color tfplan > plan_output.txt
        continue-on-error: true
      
      - name: Upload plan artifact
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan
          path: terraform/tfplan
          retention-days: 5
      
      - name: Comment PR with plan
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const planOutput = fs.readFileSync('terraform/plan_output.txt', 'utf8');
            const truncatedPlan = planOutput.length > 65000 
              ? planOutput.substring(0, 65000) + '\n\n... (truncated)'
              : planOutput;
            
            const output = `#### Terraform Plan ðŸ“– \`${{ steps.plan.outcome }}\`
            
            <details><summary>Show Plan</summary>
            
            \`\`\`terraform
            ${truncatedPlan}
            \`\`\`
            
            </details>
            
            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && comment.body.includes('Terraform Plan')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: output
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: output
              });
            }
      
      - name: Plan Status Check
        if: steps.plan.outcome == 'failure'
        run: exit 1

  # Apply job
  apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: validate
    if: |
      (github.ref == 'refs/heads/main' && github.event_name == 'push') || 
      (github.event_name == 'workflow_dispatch' && inputs.action == 'apply')
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false  # Disable for output parsing
      
      - name: Terraform Init
        run: terraform init
      
      - name: Terraform Apply
        id: apply
        run: |
          terraform apply -auto-approve -input=false
          terraform output -json > outputs.json
      

      - name: Parse Terraform Outputs
        id: outputs
        run: |
          CLUSTER_NAME=$(jq -r '.cluster_name.value' outputs.json)
          AUTOSCALER_ROLE_ARN=$(jq -r '.cluster_autoscaler_role_arn.value' outputs.json)
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "autoscaler_role_arn=$AUTOSCALER_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "Cluster: $CLUSTER_NAME"
          echo "Autoscaler Role ARN: $AUTOSCALER_ROLE_ARN"

      # ...existing code...
      
      - name: Configure EKS access for pipeline
        run: |
          set -e
          CURRENT_ARN=$(aws sts get-caller-identity --query Arn --output text)
          CLUSTER_NAME="${{ steps.outputs.outputs.cluster_name }}"
          
          echo "Adding IAM role to EKS cluster access..."
          
          # Create access entry (ignore if exists)
          aws eks create-access-entry \
            --cluster-name "$CLUSTER_NAME" \
            --principal-arn "$CURRENT_ARN" \
            --type STANDARD \
            --region ${{ env.AWS_REGION }} 2>&1 | grep -v "ResourceInUseException" || true
          
          # Associate admin policy
          aws eks associate-access-policy \
            --cluster-name "$CLUSTER_NAME" \
            --principal-arn "$CURRENT_ARN" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region ${{ env.AWS_REGION }} 2>&1 | grep -v "ResourceInUseException" || true
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name ${{ steps.outputs.outputs.cluster_name }}
          
          # Verify connectivity
          kubectl cluster-info
          kubectl get nodes
      
      # - name: Install Cluster Autoscaler
      #   run: |
      #     set -e
      #     
      #     # Install Cluster Autoscaler for automatic node scaling
      #     helm repo add autoscaler https://kubernetes.github.io/autoscaler
      #     helm repo update
      #     
      #     helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
      #       --namespace kube-system \
      #       --set autoDiscovery.clusterName=${{ steps.outputs.outputs.cluster_name }} \
      #       --set awsRegion=${{ env.AWS_REGION }} \
      #       --set rbac.serviceAccount.create=true \
      #       --set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ steps.outputs.outputs.autoscaler_role_arn }}" \
      #       --set extraArgs.balance-similar-node-groups=true \
      #       --set extraArgs.skip-nodes-with-system-pods=false \
      #       --set extraArgs.scale-down-utilization-threshold=0.7 \
      #       --set extraArgs.scale-down-unneeded-time=2m \
      #       --wait --timeout=600s
      #     
      #     echo "Cluster Autoscaler installed successfully"
      
      - name: Create Namespaces
        run: |
          kubectl apply -f ../k8s/namespaces/namespaces.yaml
          kubectl get namespaces
      
      - name: Install cert-manager
        run: |
          set -e
          helm repo add jetstack https://charts.jetstack.io
          helm repo update
          
          # Check if already installed
          if helm list -n cert-manager | grep -q cert-manager; then
            echo "cert-manager already installed, upgrading..."
            INSTALL_CMD="upgrade"
          else
            echo "Installing cert-manager..."
            INSTALL_CMD="install"
          fi
          
          helm $INSTALL_CMD cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --values ../k8s/tls/cert-manager-values.yaml \
            --wait --timeout=300s
          
          # Verify installation
          kubectl wait --for=condition=Available deployment/cert-manager \
            -n cert-manager --timeout=300s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook \
            -n cert-manager --timeout=300s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector \
            -n cert-manager --timeout=300s
          
          echo "cert-manager installed successfully"
      
      - name: Install Prometheus CRDs
        run: |
          set -e
          
          # Install only ServiceMonitor CRD (needed for ingress controller)
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
          
          # Wait for CRD to be established
          kubectl wait --for=condition=established --timeout=60s crd/servicemonitors.monitoring.coreos.com
          
          echo "ServiceMonitor CRD installed successfully"
      
      - name: Install NGINX Ingress Controller
        run: |
          set -e
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.service.type=LoadBalancer \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-cross-zone-load-balancing-enabled"=true \
            --set controller.metrics.enabled=true \
            --set controller.metrics.serviceMonitor.enabled=true \
            --set controller.podAnnotations."prometheus\.io/scrape"=true \
            --set controller.podAnnotations."prometheus\.io/port"=10254 \
            --wait --timeout=600s
          
          echo "Waiting for LoadBalancer provisioning..."
          kubectl wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=600s
      
      - name: Configure TLS certificates
        run: |
          set -e
          
          # Get LoadBalancer DNS
          LB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          if [ -z "$LB_DNS" ]; then
            echo "::error::Failed to get LoadBalancer DNS"
            exit 1
          fi
          
          echo "LoadBalancer DNS: $LB_DNS"
          echo "lb_dns=$LB_DNS" >> $GITHUB_OUTPUT
          
          # Apply Let's Encrypt issuers and certificates
          kubectl apply -f ../k8s/tls/letsencrypt-certificates.yaml
          
          # Wait for certificates to be ready (with timeout)
          echo "Waiting for certificates to be issued..."
          for i in {1..30}; do
            READY=$(kubectl get certificates -A -o json | \
              jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="True")) | .metadata.name' | wc -l)
            TOTAL=$(kubectl get certificates -A --no-headers | wc -l)
            echo "Certificates ready: $READY/$TOTAL"
            
            if [ "$READY" -eq "$TOTAL" ] && [ "$TOTAL" -gt 0 ]; then
              echo "All certificates issued successfully"
              break
            fi
            
            if [ "$i" -eq 30 ]; then
              echo "::warning::Timeout waiting for certificates. Check manually."
              kubectl get certificates -A
            fi
            
            sleep 10
          done
      
      - name: Install Prometheus & Grafana
        env:
          GRAFANA_ADMIN_PASSWORD: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}
        run: |
          set -e
          
          if [ -z "$GRAFANA_ADMIN_PASSWORD" ]; then
            echo "::error::GRAFANA_ADMIN_PASSWORD secret not set"
            exit 1
          fi
          
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set kubeStateMetrics.enabled=true \
            --set nodeExporter.enabled=true \
            --set grafana.enabled=true \
            --set prometheus.enabled=true \
            --set alertmanager.enabled=false \
            --set prometheusOperator.enabled=true \
            --set defaultRules.enabled=true \
            --set crds.enabled=true \
            --values ../k8s/monitoring/prometheus-values.yaml \
            --set grafana.adminPassword="$GRAFANA_ADMIN_PASSWORD" \
            --wait --timeout=600s
          
          echo "Monitoring stack installed successfully"
      
      - name: Apply Grafana Ingress
        run: |
          kubectl apply -f ../k8s/ingress/grafana-ingress.yaml
          echo "Grafana ingress applied successfully"
      
      - name: Deployment Summary
        run: |
          echo "## ðŸš€ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Cluster Information" >> $GITHUB_STEP_SUMMARY
          echo "- **Cluster Name**: ${{ steps.outputs.outputs.cluster_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Region**: ${{ env.AWS_REGION }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          LB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          echo "### LoadBalancer" >> $GITHUB_STEP_SUMMARY
          echo "- **DNS**: \`$LB_DNS\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Required DNS Configuration" >> $GITHUB_STEP_SUMMARY
          echo "Point these domains to the LoadBalancer:" >> $GITHUB_STEP_SUMMARY
          echo "- grafana.nainika.store â†’ $LB_DNS" >> $GITHUB_STEP_SUMMARY
          echo "- prod.nainika.store â†’ $LB_DNS" >> $GITHUB_STEP_SUMMARY
          echo "- stage.nainika.store â†’ $LB_DNS" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Installed Components" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… EKS Cluster" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… cert-manager" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… NGINX Ingress Controller" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Prometheus & Grafana" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Cluster Autoscaler" >> $GITHUB_STEP_SUMMARY

  # Destroy job
  destroy:
    name: Terraform Destroy
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && inputs.action == 'destroy'
    environment: 
      name: production-destroy
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Init
        run: terraform init
      
      - name: Cleanup Kubernetes resources
        continue-on-error: true
        run: |
          # Try to cleanup Helm releases before destroying infrastructure
          CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "")
          
          if [ -n "$CLUSTER_NAME" ]; then
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name "$CLUSTER_NAME" || true
            
            helm uninstall kube-prometheus-stack -n monitoring || true
            helm uninstall cluster-autoscaler -n kube-system || true
            helm uninstall ingress-nginx -n ingress-nginx || true
            helm uninstall cert-manager -n cert-manager || true
            
            # Wait for LoadBalancer deletion
            sleep 30
          fi
      
      - name: Terraform Destroy
        run: terraform destroy -auto-approve
      
      - name: Destroy Summary
        run: |
          echo "## ðŸ—‘ï¸ Destroy Complete" >> $GITHUB_STEP_SUMMARY
          echo "All infrastructure has been destroyed." >> $GITHUB_STEP_SUMMARY