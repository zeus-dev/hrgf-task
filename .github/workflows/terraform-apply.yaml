name: Terraform - Provision EKS

on:
  push:
    branches:
      - main
    paths:
      - 'terraform/**'
  pull_request:
    branches:
      - main
    paths:
      - 'terraform/**'
  workflow_dispatch:

env:
  AWS_REGION: ap-south-1
  TF_VERSION: 1.6.0

jobs:
  terraform:
    name: Terraform Apply
    runs-on: ubuntu-latest
    
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Format Check
        id: fmt
        run: terraform fmt -check
        continue-on-error: true
      
      - name: Terraform Init
        id: init
        run: terraform init
      
      - name: Terraform Validate
        id: validate
        run: terraform validate -no-color
      
      - name: Terraform Plan
        id: plan
        if: github.event_name == 'pull_request'
        run: terraform plan -no-color -input=false
        continue-on-error: true
      
      - name: Update Pull Request
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        env:
          PLAN: ${{ steps.plan.outputs.stdout }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### Terraform Format and Style üñå\`${{ steps.fmt.outcome }}\`
            #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
            #### Terraform Validation ü§ñ\`${{ steps.validate.outcome }}\`
            #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`
            
            <details><summary>Show Plan</summary>
            
            \`\`\`terraform\n
            ${process.env.PLAN}
            \`\`\`
            
            </details>
            
            *Pushed by: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
      
      - name: Terraform Plan Status
        if: steps.plan.outcome == 'failure'
        run: exit 1
      
      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Import existing resources to avoid conflicts
          terraform import aws_iam_policy.aws_load_balancer_controller arn:aws:iam::148450584786:policy/nasa-eks-aws-load-balancer-controller || true
          terraform import module.eks.module.kms.aws_kms_alias.this[\"cluster\"] alias/eks/nasa-eks || true
          terraform import module.eks.aws_cloudwatch_log_group.this[0] /aws/eks/nasa-eks/cluster || true
          
          terraform apply -auto-approve -input=false
      
      - name: Get EKS cluster name
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        id: cluster
        run: |
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT
      
      - name: Get current IAM identity
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        id: iam_identity
        run: |
          CURRENT_ARN=$(aws sts get-caller-identity --query Arn --output text)
          echo "current_arn=$CURRENT_ARN" >> $GITHUB_OUTPUT
      
      - name: Add pipeline IAM role to EKS access
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Create access entry for the pipeline IAM role
          aws eks create-access-entry \
            --cluster-name ${{ steps.cluster.outputs.cluster_name }} \
            --principal-arn ${{ steps.iam_identity.outputs.current_arn }} \
            --type STANDARD \
            --region ${{ env.AWS_REGION }} || true
          
          # Associate admin policy with the access entry
          aws eks associate-access-policy \
            --cluster-name ${{ steps.cluster.outputs.cluster_name }} \
            --principal-arn ${{ steps.iam_identity.outputs.current_arn }} \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region ${{ env.AWS_REGION }} || true
      
      - name: Update kubeconfig
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ steps.cluster.outputs.cluster_name }}
      
      - name: Create Namespaces
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          kubectl apply -f ../k8s/namespaces/namespaces.yaml
      
      - name: Install cert-manager
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Add cert-manager Helm repository
          helm repo add jetstack https://charts.jetstack.io
          helm repo update
          
          # Install cert-manager with CRDs
          helm upgrade --install cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --values ../k8s/tls/cert-manager-values.yaml \
            --wait --timeout=300s
          
          # Wait for cert-manager to be ready
          kubectl wait --for=condition=Available deployment/cert-manager -n cert-manager --timeout=300s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook -n cert-manager --timeout=300s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector -n cert-manager --timeout=300s
      
      - name: Install NGINX Ingress Controller
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.service.type=LoadBalancer \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-cross-zone-load-balancing-enabled"=true \
            --set controller.metrics.enabled=true \
            --set controller.metrics.serviceMonitor.enabled=true \
            --set controller.podAnnotations."prometheus\.io/scrape"=true \
            --set controller.podAnnotations."prometheus\.io/port"=10254 \
            --wait --timeout=300s
      
      - name: Wait for LoadBalancer and setup TLS
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "Waiting for LoadBalancer to be ready..."
          kubectl wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=300s
          
          # Get LoadBalancer DNS
          LB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "LoadBalancer DNS: $LB_DNS"
          echo "‚ö†Ô∏è  IMPORTANT: Point your domains to this LoadBalancer:"
          echo "   grafana.nainika.store -> $LB_DNS"
          echo "   prod.nainika.store -> $LB_DNS"
          echo "   stage.nainika.store -> $LB_DNS"
          
          # Apply Let's Encrypt cluster issuers and certificates
          kubectl apply -f ../k8s/tls/letsencrypt-certificates.yaml
      
      - name: Install Prometheus & Grafana
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        env:
          GRAFANA_ADMIN_PASSWORD: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          # Substitute the Grafana password in values file
          envsubst < ../k8s/monitoring/prometheus-values.yaml > /tmp/prometheus-values.yaml
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --values /tmp/prometheus-values.yaml \
            --wait --timeout=600s
      
      - name: Get LoadBalancer DNS
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "Waiting for LoadBalancer..."
          kubectl wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=300s
          
          LB_DNS=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "LoadBalancer DNS: $LB_DNS"
          echo "Configure this DNS in Cloudflare for nainika.store"