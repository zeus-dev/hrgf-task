# Automated Kubernetes Deployment - DevOps Take-Home Task

[![Terraform - Provision EKS](https://github.com/zeus-dev/hrgf-task/actions/workflows/terraform-apply.yaml/badge.svg)](https://github.com/zeus-dev/hrgf-task/actions/workflows/terraform-apply.yaml)
[![Build and Deploy - Production](https://github.com/zeus-dev/hrgf-task/actions/workflows/build-deploy-prod.yaml/badge.svg)](https://github.com/zeus-dev/hrgf-task/actions/workflows/build-deploy-prod.yaml)

## üéØ Overview

This project demonstrates a complete automated Kubernetes deployment pipeline for a "Nainika store" web application. The solution implements modern DevOps practices using Infrastructure as Code, containerization, Kubernetes orchestration, and automated CI/CD pipelines.

### ‚úÖ Core Requirements Implemented

1. **Infrastructure as Code**: Terraform provisions Amazon EKS cluster with VPC, networking, and security groups
2. **Containerization**: Multi-stage Dockerfile with optimized Nginx-based web application
3. **Kubernetes Manifests**: Helm charts for application deployment with LoadBalancer services and Ingress
4. **CI/CD Pipeline**: GitHub Actions automates build, security scanning, and deployment
5. **Documentation**: Comprehensive setup and usage instructions

### üéÅ Bonus Features Implemented

- **Helm Packaging**: Application deployed using Helm charts with environment-specific values
- **Secrets Management**: GitHub Secrets for AWS credentials, Docker registry, and monitoring passwords
- **Observability**: Prometheus & Grafana monitoring stack with pre-configured dashboards
- **Security**: Trivy vulnerability scanning, TLS certificates, and container security contexts

## üöÄ Live Deployed Application

- **Production**: [https://prod.nainika.store](https://prod.nainika.store)
- **Staging**: [https://stage.nainika.store](https://stage.nainika.store)
- **Monitoring**: [https://grafana.nainika.store](https://grafana.nainika.store) 

## üèóÔ∏è Architecture

```mermaid
graph TB
    subgraph "Developer"
        A[Git Push to main/develop]
    end

    subgraph "GitHub"
        B[Git Repository] --> C[GitHub Actions CI/CD]
    end

    subgraph "Docker Hub"
        D[Docker Images<br/>zeusdev27/myhello-app]
    end

    subgraph "AWS Cloud (ap-south-1)"
        subgraph "VPC & Networking"
            E[EKS Cluster<br/>nasa-eks]
            F[Node Groups<br/>t3.small instances]
            G[S3 Backend<br/>Terraform State]
            H[DynamoDB<br/>State Locking]
        end

        subgraph "Kubernetes Components"
            I[NGINX Ingress<br/>Controller]
            J[cert-manager<br/>Let's Encrypt]
        end

        subgraph "Namespaces"
            subgraph "prod"
                K[Frontend App<br/>3 replicas]
                L[Ingress<br/>prod.nainika.store]
                M[TLS Certificate<br/>Let's Encrypt]
            end
            subgraph "stage"
                N[Frontend App<br/>2 replicas]
                O[Ingress<br/>stage.nainika.store]
                P[TLS Certificate<br/>Let's Encrypt]
            end
            subgraph "monitoring"
                Q[Prometheus<br/>Metrics Collection]
                R[Grafana<br/>Dashboards]
                S[Ingress<br/>grafana.nainika.store]
                T[TLS Certificate<br/>Let's Encrypt]
            end
        end
    end

    subgraph "External Services"
        U[Let's Encrypt<br/>ACME Server]
        V[Cloudflare<br/>DNS Records<br/>Full Strict SSL]
    end

    A --> B
    C -->|Terraform Plan/Apply| E
    C -->|Docker Build & Scan| D
    C -->|Helm Deploy| K
    C -->|Helm Deploy| N
    C -->|Helm Deploy| Q
    E --> F
    E --> G
    E --> H
    I --> L
    I --> O
    I --> S
    J --> M
    J --> P
    J --> T
    J -->|Requests Certs| U
    V -->|DNS-01 Challenge| U
```

### Architecture Flow
1. **Code Push** ‚Üí GitHub repository triggers CI/CD pipeline
2. **Infrastructure** ‚Üí Terraform provisions EKS cluster, VPC, and networking
3. **Container Build** ‚Üí Multi-stage Docker build with security scanning
4. **Deployment** ‚Üí Helm charts deploy applications to respective namespaces
5. **Ingress & TLS** ‚Üí NGINX controller routes traffic with Let's Encrypt certificates
6. **Monitoring** ‚Üí Prometheus collects metrics, Grafana provides dashboards

### Key Components
- **Multi-environment**: Separate prod/stage namespaces with dedicated resources
- **Security**: TLS encryption, vulnerability scanning, RBAC, non-root containers
- **Scalability**: Configurable replicas, resource limits, horizontal pod autoscaling ready
- **Observability**: Comprehensive monitoring with Prometheus metrics and Grafana visualization
- **Automation**: Fully automated CI/CD with security gates and rollback capabilities

### Generate PNG Diagram
To generate a PNG image of the architecture diagram:
```bash
# Install Mermaid CLI
npm install -g @mermaid-js/mermaid-cli

# Generate PNG from Mermaid file
mmdc -i architecture.mmd -o architecture.png -t dark -b transparent
```

The `architecture.mmd` file is included in the repository root for PNG generation.

## üõ†Ô∏è Technology Stack

- **Cloud Provider**: AWS (ap-south-1 region)
- **Infrastructure**: Terraform, Amazon EKS
- **Containerization**: Docker, Docker Hub Registry
- **Orchestration**: Kubernetes, Helm
- **CI/CD**: GitHub Actions
- **Monitoring**: Prometheus, Grafana
- **Ingress**: NGINX Ingress Controller
- **DNS & TLS**: Cloudflare
- **Security**: cert-manager, RBAC, Security Contexts

## üìã Prerequisites

- AWS CLI configured with EKS permissions
- Docker installed locally
- kubectl installed
- Helm 3.x installed
- Terraform >= 1.0
- GitHub repository with secrets configured

### Required GitHub Secrets
```
AWS_ACCESS_KEY_ID          # AWS credentials
AWS_SECRET_ACCESS_KEY      # AWS credentials
DOCKER_HUB_USERNAME        # Docker Hub registry
DOCKER_HUB_ACCESS_TOKEN    # Docker Hub access token
GRAFANA_ADMIN_PASSWORD     # Monitoring admin password
```

## üöÄ Quick Start

### 1. Clone and Setup
```bash
git clone https://github.com/zeus-dev/hrgf-task.git
cd hrgf-task
```

### 2. Infrastructure Provisioning
```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 3. Configure Kubernetes Access
```bash
aws eks update-kubeconfig --region ap-south-1 --name nasa-eks
```

### 4. Deploy Application
The CI/CD pipeline automatically deploys on git push. For manual deployment:
```bash
# Deploy to staging
helm upgrade --install frontend-app-stage ./k8s/helm/frontend-app \
  -f ./k8s/helm/frontend-app/value-stage.yaml -n stage --create-namespace

# Deploy to production
helm upgrade --install frontend-app-prod ./k8s/helm/frontend-app \
  -f ./k8s/helm/frontend-app/value-prod.yaml -n prod --create-namespace
```

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ frontend/                 # Web application source
‚îÇ   ‚îú‚îÄ‚îÄ src/index.html       # Simple HTML application
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile          # Multi-stage container build
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf          # NGINX web server config
‚îÇ   ‚îî‚îÄ‚îÄ package.json        # Node.js dependencies
‚îú‚îÄ‚îÄ terraform/              # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ main.tf            # EKS cluster configuration
‚îÇ   ‚îú‚îÄ‚îÄ vpc.tf             # VPC and networking setup
‚îÇ   ‚îú‚îÄ‚îÄ eks.tf             # EKS-specific resources
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf       # Input variables
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf         # Output values
‚îÇ   ‚îî‚îÄ‚îÄ backend.tf         # S3 backend configuration
‚îú‚îÄ‚îÄ k8s/                   # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ helm/frontend-app/ # Helm chart for application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml    # Chart metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values.yaml   # Default values
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ value-prod.yaml  # Production overrides
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ value-stage.yaml # Staging overrides
‚îÇ   ‚îú‚îÄ‚îÄ namespaces/       # Namespace definitions
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/       # Prometheus & Grafana config
‚îÇ   ‚îî‚îÄ‚îÄ tls/              # Certificate configurations
‚îî‚îÄ‚îÄ .github/workflows/    # CI/CD pipelines
    ‚îú‚îÄ‚îÄ terraform-apply.yaml     # Infrastructure pipeline
    ‚îú‚îÄ‚îÄ build-deploy-prod.yaml   # Production deployment
    ‚îî‚îÄ‚îÄ build-deploy-stage.yaml  # Staging deployment
```

## üîÑ CI/CD Pipeline

### Infrastructure Pipeline (`terraform-apply.yaml`)
- **Trigger**: Changes to `terraform/` directory
- **Stages**:
  1. Validate Terraform syntax
  2. Plan infrastructure changes
  3. Apply changes to AWS (manual approval required)
  4. Deploy Kubernetes components (cert-manager, ingress, monitoring)

### Application Deployment Pipelines
- **Trigger**: Push to `main` (production) or `develop` (staging)
- **Stages**:
  1. Build Docker image with multi-stage optimization
  2. Security scan with Trivy (vulnerability detection)
  3. Push image to Docker Hub registry
  4. Deploy to Kubernetes using Helm
  5. Health check and verification

## üé® Design Choices & Rationale

### Infrastructure Decisions
- **AWS EKS**: Managed Kubernetes service reduces operational overhead
- **Terraform**: Declarative IaC ensures reproducible infrastructure
- **S3 Backend**: Remote state management with DynamoDB locking
- **VPC Design**: Private subnets for security, NAT gateways for outbound traffic

### Application Architecture
- **Multi-stage Docker**: Reduces image size and attack surface
- **Nginx Base**: Lightweight, high-performance web server
- **Read-only Filesystem**: Enhanced security with immutable containers
- **Non-root User**: Security best practice to prevent privilege escalation

### Deployment Strategy
- **Helm Charts**: Templated deployments with environment-specific values
- **Separate Namespaces**: Environment isolation (prod/stage/monitoring)
- **Rolling Updates**: Zero-downtime deployments with health checks
- **Resource Limits**: Cost optimization and resource protection

### Security Measures
- **Vulnerability Scanning**: Automated Trivy scans in CI/CD
- **TLS Everywhere**: Let's Encrypt certificates for all endpoints
- **RBAC**: Kubernetes role-based access control
- **Secrets Management**: GitHub Secrets for sensitive credentials

### Monitoring & Observability
- **Prometheus Stack**: Industry-standard monitoring solution
- **Pre-built Dashboards**: Grafana dashboards for common metrics
- **Service Discovery**: Automatic monitoring of Kubernetes services
- **Alerting Ready**: Prometheus rules configured for alerting

## üìä Monitoring Dashboard

Access the monitoring stack at [https://grafana.nainika.store](https://grafana.nainika.store)

**Available Dashboards:**
- Kubernetes Cluster Monitoring
- NGINX Ingress Controller
- Node Exporter (System Metrics)
- Pod Resource Usage
- Prometheus Server Health

## üîß Troubleshooting

### Check Application Status
```bash
# View all pods
kubectl get pods -A

# Check application logs
kubectl logs -f deployment/frontend-app-prod -n prod

# Verify ingress
kubectl get ingress -A
```

### Common Issues
1. **Image Pull Errors**: Check Docker Hub credentials in GitHub secrets
2. **TLS Certificate Pending**: Wait for Let's Encrypt validation (may take 5-10 minutes)
3. **Pod Resource Limits**: Adjust resource requests/limits in Helm values
4. **Ingress Not Working**: Verify ingress controller is running in ingress-nginx namespace

## üìà Performance & Cost Optimization

### Resource Allocation
- **Production**: 3 replicas, 500m CPU, 512Mi RAM each
- **Staging**: 2 replicas, 300m CPU, 256Mi RAM each
- **Monitoring**: Optimized for minimal resource usage

### Cost Considerations
- EKS cluster uses t3.medium instances (free tier eligible)
- S3 backend for Terraform state (low cost)
- Docker Hub free tier for container registry
- Let's Encrypt free SSL certificates

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/new-feature`)
3. Make changes and test locally
4. Commit changes (`git commit -m 'Add new feature'`)
5. Push to branch (`git push origin feature/new-feature`)
6. Create Pull Request with description

## üìú License

MIT License - see [LICENSE](LICENSE) file for details.

---

**üéØ This implementation demonstrates production-ready DevOps practices with automated infrastructure provisioning, secure container deployments, comprehensive monitoring, and robust CI/CD pipelines.**

## üìã Overview

This project demonstrates a complete cloud-native infrastructure setup featuring:

- **Infrastructure as Code**: Terraform for AWS EKS provisioning
- **Containerization**: Optimized Docker containers with security best practices
- **Kubernetes Orchestration**: Helm charts for application deployment
- **CI/CD Pipeline**: GitHub Actions for automated deployments
- **Monitoring Stack**: Prometheus & Grafana with pre-configured dashboards
- **Security**: Cloudflare TLS certificates, container security contexts
- **Multi-Environment**: Separate staging and production environments

##  Project Structure

```
.
‚îú‚îÄ‚îÄ frontend/                 # Frontend application
‚îÇ   ‚îú‚îÄ‚îÄ src/                 # HTML/CSS/JS source code
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile           # Multi-stage container build
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf           # NGINX configuration
‚îÇ   ‚îî‚îÄ‚îÄ package.json         # Dependencies and scripts
‚îú‚îÄ‚îÄ terraform/               # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # EKS and addon configurations
‚îÇ   ‚îú‚îÄ‚îÄ vpc.tf              # VPC and networking
‚îÇ   ‚îú‚îÄ‚îÄ eks.tf              # EKS cluster configuration
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf        # Input variables
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf          # Output values
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars    # Environment-specific values
‚îú‚îÄ‚îÄ k8s/                    # Kubernetes configurations
‚îÇ   ‚îú‚îÄ‚îÄ helm/frontend-app/  # Helm chart for application
‚îÇ   ‚îú‚îÄ‚îÄ namespaces/         # Namespace definitions
‚îÇ   ‚îú‚îÄ‚îÄ ingress/            # Ingress configurations
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/         # Prometheus & Grafana setup
‚îÇ   ‚îî‚îÄ‚îÄ tls/                # TLS certificate configurations
‚îî‚îÄ‚îÄ .github/workflows/      # CI/CD pipelines
    ‚îú‚îÄ‚îÄ terraform-apply.yaml
    ‚îú‚îÄ‚îÄ build-deploy-prod.yaml
    ‚îî‚îÄ‚îÄ build-deploy-stage.yaml
```

## üöÄ Getting Started

### Prerequisites

- AWS CLI configured with appropriate permissions
- Docker installed locally
- kubectl installed
- Helm 3.x installed
- Terraform >= 1.0
- GitHub account with repository secrets configured

### Required GitHub Secrets

```bash
# AWS Credentials
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY

# Docker Hub Credentials
DOCKER_HUB_USERNAME
DOCKER_HUB_ACCESS_TOKEN

# Cloudflare API Token (for TLS certificates)
CLOUDFLARE_API_TOKEN
```

### 1. Infrastructure Setup

```bash
# Clone the repository
git clone https://github.com/zeus-dev/hrgf-task.git
cd hrgf-task

# Setup Terraform backend (one-time setup)
cd terraform
aws s3api create-bucket --bucket nainika-terraform-state --region ap-south-1 \
  --create-bucket-configuration LocationConstraint=ap-south-1
aws s3api put-bucket-versioning --bucket nainika-terraform-state \
  --versioning-configuration Status=Enabled
aws dynamodb create-table --table-name terraform-state-lock \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST --region ap-south-1

# Uncomment the backend configuration in backend.tf
# Then initialize and apply Terraform
terraform init
terraform plan
terraform apply
```

### 2. Configure kubectl

```bash
aws eks update-kubeconfig --region ap-south-1 --name nasa-eks
```

### 3. Install Required Kubernetes Components

```bash
# Install NGINX Ingress Controller
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx --create-namespace

# Install cert-manager for TLS certificates
helm repo add jetstack https://charts.jetstack.io
helm upgrade --install cert-manager jetstack/cert-manager \
  --namespace cert-manager --create-namespace \
  --set installCRDs=true

# Apply TLS certificate configuration
kubectl apply -f k8s/tls/cloudflare-certificates.yaml

# Install monitoring stack
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
  -f k8s/monitoring/prometheus-values.yaml \
  -n monitoring --create-namespace

# Or use the automated deployment script
./deploy-monitoring.sh
```

### 4. Deploy Application

```bash
# Create namespaces
kubectl apply -f k8s/namespaces/namespaces.yaml

# Deploy to staging
helm upgrade --install frontend-app-stage ./k8s/helm/frontend-app \
  -f ./k8s/helm/frontend-app/value-stage.yaml -n stage

# Deploy to production
helm upgrade --install frontend-app-prod ./k8s/helm/frontend-app \
  -f ./k8s/helm/frontend-app/value-prod.yaml -n prod
```

## üîÑ CI/CD Pipeline

The project includes three automated workflows:

### 1. Infrastructure Pipeline (`terraform-apply.yaml`)
- **Trigger**: Changes to `terraform/` directory
- **Actions**: Validates, plans, and applies Terraform changes
- **Environment**: AWS EKS cluster provisioning

### 2. Production Deployment (`build-deploy-prod.yaml`)
- **Trigger**: Push to `main` branch with frontend changes
- **Actions**: Build ‚Üí Security Scan ‚Üí Push to Docker Hub ‚Üí Deploy to production
- **Environment**: Production namespace
- **Security**: Trivy vulnerability scanning with results uploaded to GitHub Security tab

### 3. Staging Deployment (`build-deploy-stage.yaml`)
- **Trigger**: Push to `develop` branch with frontend changes
- **Actions**: Build ‚Üí Security Scan ‚Üí Push to Docker Hub ‚Üí Deploy to staging
- **Environment**: Staging namespace
- **Security**: Trivy vulnerability scanning with results uploaded to GitHub Security tab

## üìä Monitoring & Observability

### Pre-configured Grafana Dashboards

1. **NGINX Ingress Controller** (Dashboard ID: 14314)
   - Request rates, response times, error rates
   - Ingress resource monitoring

2. **Kubernetes Cluster Monitoring** (Dashboard ID: 7249)
   - Node resource utilization
   - Pod lifecycle and health

3. **Kubernetes Pods Monitoring** (Dashboard ID: 6417)
   - Container metrics and resource usage
   - Pod restart and failure tracking

4. **Node Exporter Full** (Dashboard ID: 1860)
   - System-level metrics for all nodes
   - CPU, memory, disk, and network monitoring

5. **Prometheus 2.0 Overview** (Dashboard ID: 3662)
   - Prometheus server health and performance
   - Query performance and storage metrics

### Access Monitoring

```bash
# Get Grafana admin password
kubectl get secret --namespace monitoring prometheus-grafana \
  -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

# Port-forward for local access (if needed)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

## üîí Security Features

- **Container Security**: Non-root containers, read-only root filesystem
- **Vulnerability Scanning**: Trivy integration in CI/CD pipelines for container image security
- **Network Security**: NGINX Ingress with proper security headers
- **TLS Encryption**: Cloudflare-managed SSL/TLS certificates
- **RBAC**: Kubernetes role-based access control
- **Secrets Management**: Kubernetes secrets for sensitive data
- **Image Security**: Multi-stage Docker builds with minimal attack surface

## üåç Environment Configuration

### Production Environment
- **URL**: https://nainika.store
- **Replicas**: 3 pods
- **Resources**: 500m CPU, 512Mi memory per pod
- **Auto-scaling**: Enabled (3-10 pods based on CPU usage)

### Staging Environment
- **URL**: https://staging.nainika.store
- **Replicas**: 2 pods
- **Resources**: 300m CPU, 256Mi memory per pod
- **Auto-scaling**: Disabled for cost optimization

## üõ†Ô∏è Local Development

```bash
# Run frontend locally
cd frontend
npm install
npm start
# Access at http://localhost:8080

# Build Docker image locally
docker build -t zeusdev27/myhello-app:local .
docker run -p 8080:8080 zeusdev27/myhello-app:local

# Test Kubernetes manifests
helm template ./k8s/helm/frontend-app -f ./k8s/helm/frontend-app/values.yaml
```

## üîß Troubleshooting

### Common Issues

1. **TLS Certificate Issues**
   ```bash
   # Check certificate status
   kubectl get certificates -A
   kubectl describe certificate nainika-store-cert -n prod
   ```

2. **Pod Not Starting**
   ```bash
   # Check pod logs
   kubectl logs -f deployment/frontend-app-prod -n prod
   kubectl describe pod <pod-name> -n prod
   ```

3. **Ingress Not Working**
   ```bash
   # Check ingress controller
   kubectl get pods -n ingress-nginx
   kubectl logs -f deployment/ingress-nginx-controller -n ingress-nginx
   ```

### Useful Commands

```bash
# Check cluster status
kubectl get nodes
kubectl get namespaces

# Monitor deployments
kubectl get deployments -A
kubectl get pods -A

# Check services and ingress
kubectl get svc -A
kubectl get ingress -A

# View resource usage
kubectl top nodes
kubectl top pods -A
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë• Maintainers

- **DevOps Team** - [devops@nainika.store](mailto:devops@nainika.store)

---

**Note**: This project is optimized for AWS free tier usage with cost-effective resource allocation. For production workloads at scale, consider adjusting instance types and resource limits accordingly.
